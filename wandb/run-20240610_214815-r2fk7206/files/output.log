d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
d:\newenv\lib\site-packages\transformers\models\llama\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
{'loss': 1.461, 'grad_norm': 1.5000876188278198, 'learning_rate': 0.0001996574826304608, 'epoch': 0.03}
{'loss': 1.2253, 'grad_norm': 2.0260889530181885, 'learning_rate': 0.00019862310329196405, 'epoch': 0.05}
{'loss': 1.1824, 'grad_norm': 1.4600756168365479, 'learning_rate': 0.0001969040277815861, 'epoch': 0.08}
{'loss': 1.1457, 'grad_norm': 1.2228676080703735, 'learning_rate': 0.00019451219087304468, 'epoch': 0.11}
{'loss': 1.1082, 'grad_norm': 1.3293144702911377, 'learning_rate': 0.0001914641980220635, 'epoch': 0.13}
{'loss': 1.0657, 'grad_norm': 1.6991515159606934, 'learning_rate': 0.00018778121008210708, 'epoch': 0.16}
d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0654, 'grad_norm': 1.659189224243164, 'learning_rate': 0.00018348879639402363, 'epoch': 0.19}
{'loss': 1.0425, 'grad_norm': 2.6222612857818604, 'learning_rate': 0.00017861675726952772, 'epoch': 0.21}
{'loss': 1.0111, 'grad_norm': 2.076979637145996, 'learning_rate': 0.00017319891710094174, 'epoch': 0.24}
{'loss': 0.9864, 'grad_norm': 2.320526123046875, 'learning_rate': 0.00016727288953354217, 'epoch': 0.27}
{'loss': 0.9659, 'grad_norm': 2.1629951000213623, 'learning_rate': 0.00016087981633081474, 'epoch': 0.29}
{'loss': 0.9341, 'grad_norm': 2.3726818561553955, 'learning_rate': 0.00015406408174555976, 'epoch': 0.32}
{'loss': 0.9099, 'grad_norm': 2.5699565410614014, 'learning_rate': 0.0001468730043798422, 'epoch': 0.34}
d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.8873, 'grad_norm': 2.5225443840026855, 'learning_rate': 0.00013935650867306473, 'epoch': 0.37}
{'loss': 0.8775, 'grad_norm': 2.396089553833008, 'learning_rate': 0.0001315667782988759, 'epoch': 0.4}
{'loss': 0.8372, 'grad_norm': 3.0049238204956055, 'learning_rate': 0.0001235578938772234, 'epoch': 0.42}
{'loss': 0.8478, 'grad_norm': 2.48687744140625, 'learning_rate': 0.00011538545751675627, 'epoch': 0.45}
{'loss': 0.8271, 'grad_norm': 3.1567542552948, 'learning_rate': 0.00010710620679421031, 'epoch': 0.48}
{'loss': 0.8077, 'grad_norm': 3.2829158306121826, 'learning_rate': 9.883318095760817e-05, 'epoch': 0.5}
d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.7697, 'grad_norm': 2.38227915763855, 'learning_rate': 9.051283332384336e-05, 'epoch': 0.53}
{'loss': 0.7651, 'grad_norm': 3.521379232406616, 'learning_rate': 8.231303618442691e-05, 'epoch': 0.56}
{'loss': 0.7179, 'grad_norm': 4.623102188110352, 'learning_rate': 7.418071682477616e-05, 'epoch': 0.58}
{'loss': 0.7483, 'grad_norm': 3.0657029151916504, 'learning_rate': 6.622764922014729e-05, 'epoch': 0.61}
{'loss': 0.7107, 'grad_norm': 3.574406147003174, 'learning_rate': 5.8509047967950445e-05, 'epoch': 0.64}
{'loss': 0.7299, 'grad_norm': 5.132853031158447, 'learning_rate': 5.107849987039782e-05, 'epoch': 0.66}
{'loss': 0.6982, 'grad_norm': 6.270503044128418, 'learning_rate': 4.398759190526181e-05, 'epoch': 0.69}
d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.687, 'grad_norm': 3.9747824668884277, 'learning_rate': 3.728555308052162e-05, 'epoch': 0.72}
{'loss': 0.7036, 'grad_norm': 3.9033186435699463, 'learning_rate': 3.101891265934101e-05, 'epoch': 0.74}
{'loss': 0.6471, 'grad_norm': 3.5955119132995605, 'learning_rate': 2.5231177128169102e-05, 'epoch': 0.77}
{'loss': 0.6821, 'grad_norm': 2.2621731758117676, 'learning_rate': 1.9962528150630045e-05, 'epoch': 0.8}
{'loss': 0.6619, 'grad_norm': 2.931366205215454, 'learning_rate': 1.52495436041734e-05, 'epoch': 0.82}
{'loss': 0.6573, 'grad_norm': 3.6703972816467285, 'learning_rate': 1.1124943636202601e-05, 'epoch': 0.85}
{'loss': 0.6647, 'grad_norm': 3.8747265338897705, 'learning_rate': 7.617363502700692e-06, 'epoch': 0.88}
d:\newenv\lib\site-packages\torch\utils\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.63, 'grad_norm': 3.8585758209228516, 'learning_rate': 4.751154766433508e-06, 'epoch': 0.9}
{'loss': 0.653, 'grad_norm': 3.253211736679077, 'learning_rate': 2.546216234922105e-06, 'epoch': 0.93}
{'loss': 0.6384, 'grad_norm': 3.5604312419891357, 'learning_rate': 1.0178558119067315e-06, 'epoch': 0.96}
{'loss': 0.6388, 'grad_norm': 2.8799359798431396, 'learning_rate': 1.7668422140603157e-07, 'epoch': 0.98}
