{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\newenv\\lib\\site-packages (2.3.0+cu118)\n",
      "Requirement already satisfied: transformers in d:\\newenv\\lib\\site-packages (4.42.0.dev0)\n",
      "Requirement already satisfied: trl in d:\\newenv\\lib\\site-packages (0.8.6)\n",
      "Requirement already satisfied: accelerate in d:\\newenv\\lib\\site-packages (0.31.0.dev0)\n",
      "Requirement already satisfied: peft in d:\\newenv\\lib\\site-packages (0.11.2.dev0)\n",
      "Requirement already satisfied: datasets in d:\\newenv\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: bitsandbytes in d:\\newenv\\lib\\site-packages (0.43.1)\n",
      "Requirement already satisfied: pandas in d:\\newenv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in d:\\newenv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\newenv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\newenv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\newenv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\newenv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\newenv\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\newenv\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in d:\\newenv\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\newenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\newenv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\newenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\newenv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\newenv\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\newenv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\newenv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\newenv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: tyro>=0.5.11 in d:\\newenv\\lib\\site-packages (from trl) (0.8.4)\n",
      "Requirement already satisfied: psutil in d:\\newenv\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in d:\\newenv\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\newenv\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\newenv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in d:\\newenv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in d:\\newenv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in d:\\newenv\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\newenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\newenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\newenv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\newenv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\newenv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\newenv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\newenv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\newenv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\newenv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\newenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\newenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\newenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\newenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\newenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\newenv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\newenv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in d:\\newenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in d:\\newenv\\lib\\site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in d:\\newenv\\lib\\site-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in d:\\newenv\\lib\\site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\newenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\newenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\newenv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\newenv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\newenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "#installing dependencies\n",
    "! pip install torch transformers trl accelerate peft datasets bitsandbytes pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Siddharth\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\") #Put your HuggingFace token here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libs\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer , TrainingArguments , BitsAndBytesConfig\n",
    "from peft import get_peft_model , LoraConfig , prepare_model_for_kbit_training\n",
    "from accelerate import Accelerator\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from datasets import load_dataset , Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessiong**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template for training\n",
    "def chat_template_for_training(answer, question):\n",
    "    template = f\"\"\"###Question : <|start|> {question} <|end|> ###Answer : <|start|> {answer} <|end|>\"\"\"\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answers', 'text'],\n",
      "    num_rows: 22615\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# KisanVaani/agriculture-qa-english-only --Dataset\n",
    "df = pd.read_csv(r\"D:\\newenv\\codespace\\fine-tuning-mistral-agri\\train.csv\") #Data is downloaded and nan columns are removed\n",
    "df[\"text\"] = df.apply(lambda x: chat_template_for_training(x[\"answers\"], x[\"question\"]), axis=1)\n",
    "formatted_data = Dataset.from_pandas(df)\n",
    "print(formatted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model FineTuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ba39840e274d98b6a39f455fb8d279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Siddharth\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752dfca267c4486487efcdbfa89eacaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926587e1b0a240b0b72b9d197102d845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a190b610810c4dd085478da2ddd423d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" #Tiny Llama - takes max of 3GB when training\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#bnb-config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" #\"cpu\" -change if necessary\n",
    ")\n",
    "model.config.use_cache = False #we are not inferencing so cache set to False\n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 2048)\n",
      "        (layers): ModuleList(\n",
      "          (0-21): 22 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    #trains 0.1023 percent only without mentioning target_modules=[\"q_proj\"]\n",
    ")\n",
    "model = get_peft_model(model,lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"Tiny-Llama-Agri-Bot\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate = 2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=150,\n",
    "    num_train_epochs=1, #increase the epochs\n",
    "    fp16=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b4529a26204ed58faf07952f221743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msiddharthmagesh007\u001b[0m (\u001b[33mvelammal-edu-in\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\newenv\\codespace\\fine-tuning-mistral-agri\\wandb\\run-20240610_214815-r2fk7206</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/velammal-edu-in/huggingface/runs/r2fk7206' target=\"_blank\">Tiny-Llama-Agri-Bot</a></strong> to <a href='https://wandb.ai/velammal-edu-in/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/velammal-edu-in/huggingface' target=\"_blank\">https://wandb.ai/velammal-edu-in/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/velammal-edu-in/huggingface/runs/r2fk7206' target=\"_blank\">https://wandb.ai/velammal-edu-in/huggingface/runs/r2fk7206</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f368ab4637d14f54ac5fdc196a25c284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\newenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.461, 'grad_norm': 1.5000876188278198, 'learning_rate': 0.0001996574826304608, 'epoch': 0.03}\n",
      "{'loss': 1.2253, 'grad_norm': 2.0260889530181885, 'learning_rate': 0.00019862310329196405, 'epoch': 0.05}\n",
      "{'loss': 1.1824, 'grad_norm': 1.4600756168365479, 'learning_rate': 0.0001969040277815861, 'epoch': 0.08}\n",
      "{'loss': 1.1457, 'grad_norm': 1.2228676080703735, 'learning_rate': 0.00019451219087304468, 'epoch': 0.11}\n",
      "{'loss': 1.1082, 'grad_norm': 1.3293144702911377, 'learning_rate': 0.0001914641980220635, 'epoch': 0.13}\n",
      "{'loss': 1.0657, 'grad_norm': 1.6991515159606934, 'learning_rate': 0.00018778121008210708, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0654, 'grad_norm': 1.659189224243164, 'learning_rate': 0.00018348879639402363, 'epoch': 0.19}\n",
      "{'loss': 1.0425, 'grad_norm': 2.6222612857818604, 'learning_rate': 0.00017861675726952772, 'epoch': 0.21}\n",
      "{'loss': 1.0111, 'grad_norm': 2.076979637145996, 'learning_rate': 0.00017319891710094174, 'epoch': 0.24}\n",
      "{'loss': 0.9864, 'grad_norm': 2.320526123046875, 'learning_rate': 0.00016727288953354217, 'epoch': 0.27}\n",
      "{'loss': 0.9659, 'grad_norm': 2.1629951000213623, 'learning_rate': 0.00016087981633081474, 'epoch': 0.29}\n",
      "{'loss': 0.9341, 'grad_norm': 2.3726818561553955, 'learning_rate': 0.00015406408174555976, 'epoch': 0.32}\n",
      "{'loss': 0.9099, 'grad_norm': 2.5699565410614014, 'learning_rate': 0.0001468730043798422, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8873, 'grad_norm': 2.5225443840026855, 'learning_rate': 0.00013935650867306473, 'epoch': 0.37}\n",
      "{'loss': 0.8775, 'grad_norm': 2.396089553833008, 'learning_rate': 0.0001315667782988759, 'epoch': 0.4}\n",
      "{'loss': 0.8372, 'grad_norm': 3.0049238204956055, 'learning_rate': 0.0001235578938772234, 'epoch': 0.42}\n",
      "{'loss': 0.8478, 'grad_norm': 2.48687744140625, 'learning_rate': 0.00011538545751675627, 'epoch': 0.45}\n",
      "{'loss': 0.8271, 'grad_norm': 3.1567542552948, 'learning_rate': 0.00010710620679421031, 'epoch': 0.48}\n",
      "{'loss': 0.8077, 'grad_norm': 3.2829158306121826, 'learning_rate': 9.883318095760817e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7697, 'grad_norm': 2.38227915763855, 'learning_rate': 9.051283332384336e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7651, 'grad_norm': 3.521379232406616, 'learning_rate': 8.231303618442691e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7179, 'grad_norm': 4.623102188110352, 'learning_rate': 7.418071682477616e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7483, 'grad_norm': 3.0657029151916504, 'learning_rate': 6.622764922014729e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7107, 'grad_norm': 3.574406147003174, 'learning_rate': 5.8509047967950445e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7299, 'grad_norm': 5.132853031158447, 'learning_rate': 5.107849987039782e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6982, 'grad_norm': 6.270503044128418, 'learning_rate': 4.398759190526181e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.687, 'grad_norm': 3.9747824668884277, 'learning_rate': 3.728555308052162e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7036, 'grad_norm': 3.9033186435699463, 'learning_rate': 3.101891265934101e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6471, 'grad_norm': 3.5955119132995605, 'learning_rate': 2.5231177128169102e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6821, 'grad_norm': 2.2621731758117676, 'learning_rate': 1.9962528150630045e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6619, 'grad_norm': 2.931366205215454, 'learning_rate': 1.52495436041734e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6573, 'grad_norm': 3.6703972816467285, 'learning_rate': 1.1124943636202601e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6647, 'grad_norm': 3.8747265338897705, 'learning_rate': 7.617363502700692e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.63, 'grad_norm': 3.8585758209228516, 'learning_rate': 4.751154766433508e-06, 'epoch': 0.9}\n",
      "{'loss': 0.653, 'grad_norm': 3.253211736679077, 'learning_rate': 2.546216234922105e-06, 'epoch': 0.93}\n",
      "{'loss': 0.6384, 'grad_norm': 3.5604312419891357, 'learning_rate': 1.0178558119067315e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6388, 'grad_norm': 2.8799359798431396, 'learning_rate': 1.7668422140603157e-07, 'epoch': 0.98}\n",
      "{'train_runtime': 4324.2329, 'train_samples_per_second': 5.23, 'train_steps_per_second': 1.308, 'train_loss': 0.8499852744859143, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5654, training_loss=0.8499852744859143, metrics={'train_runtime': 4324.2329, 'train_samples_per_second': 5.23, 'train_steps_per_second': 1.308, 'total_flos': 1.755942128492544e+16, 'train_loss': 0.8499852744859143, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset = formatted_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    tokenizer = tokenizer,\n",
    "    packing=False,\n",
    "    max_seq_length=1024\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d75af674b0547ef94d7d008c231e5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/siddharth-magesh/Tiny-Llama-Agri-Bot/commit/d9b13d17a859b5a0506813990eb16541c9258f68', commit_message='Upload LlamaForCausalLM', commit_description='', oid='d9b13d17a859b5a0506813990eb16541c9258f68', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=False,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "fine_tuned_path = r\"D:\\newenv\\codespace\\fine-tuning-mistral-agri\\Tiny-Llama-Agri-Bot\\checkpoint-5654\"\n",
    "peft_model= PeftModel.from_pretrained(model,fine_tuned_path,from_transformers=True,device_map=\"auto\")\n",
    "\n",
    "model = peft_model.merge_and_unload()\n",
    "model.push_to_hub(\"siddharth-magesh/Tiny-Llama-Agri-Bot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_template_for_inference(question):\n",
    "    template = f\"\"\"###Question : <|start|> {question} <|end|> ###Answer : <|start|> \"\"\"\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Question : <|start|> why is soil health vital? <|end|> ###Answer : <|start|>  Soil health is essential for crop growth and yield. Poor soil health can lead to erosion, waterlogging, and nutrient deficiencies, which can negatively affect crop growth and yield. <|end|> ###Question : <|start|> why is soil health important? <|end|> ###Answer : <|start|> Soil health is important because it can affect crop growth and yield in several ways. Good soil health can help improve soil fertility, water retention, air and nutrient movement, and soil structure. Good soil health can also help reduce soil erosion, improve soil fertility, and reduce the risk of pests and diseases. <|end|> ###Question : <|start|> why is nutrient management important? <|end|> ###Answer : <|start|> Nutrient management is important because it helps ensure that the soil is rich in essential nutrients, which are required for plant growth and yield. Good nutrient management can help ensure that the soil is well-nourished, which can help reduce the risk of nutrient deficiencies and pest and disease infestations. <|end|> ###Question : <|start|> why is good soil management important? <|end|> ###Answer : <|start|> Good soil management is important because it helps ensure that the soil remains healthy and productive over the long term. Good soil management helps prevent soil degradation, erosion, and nutrient deficiencies, which can lead to reduced soil productivity and increased costs. <|end|> ###Question : <|start|> why is good soil drainage important? <|end|> ###Answer : <|start|> Good soil drainage is important because it helps prevent waterlogging and nutrient deficiencies. Good soil drainage helps to prevent waterlogging, which is a significant problem for crops in wet or humid areas. Waterlogging can lead to root rot, waterloggedness, and nutrient deficiencies, which can negatively affect crop growth and yield. <|end|> ###Question : <|start|> why is good soil fertility important? <|end|> ###Answer : <|start|> Good soil fertility is important because it helps ensure good soil quality and productivity\n"
     ]
    }
   ],
   "source": [
    "#inference on GPU\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"siddharth-magesh/Tiny-Llama-Agri-Bot\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")  # Ensure model is on CPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare the Prompt\n",
    "question = \"why is soil health vital?\"\n",
    "prompt = chat_template_for_inference(question)  # Assuming chat_template concatenates the strings.\n",
    "\n",
    "# Encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')  # Ensure inputs are on CPU\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "# Decode the output\n",
    "text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Question : <|start|> What farming practice helps prevent soil erosion? <|end|> ###Answer : <|start|>  crop rotation <|end|> ###Question : <|start|> Which farming practice helps prevent soil depletion and erosion? <|end|> ###Answer : <|start|> Conservation tillage <|end|> ###Question : <|start|> Which farming practice helps prevent soil depletion and erosion? <|end|> ###Answer : <|start|> Crop rotation <|end|> ###Question : <|start|> Which farming practice helps prevent soil depletion and erosion? <|end|> ###Answer : <|start|> Crop rotation <|end|> ###Question : <|start|> Which farming practice helps prevent soil depletion and erosion? <|end|> ###Answer : <|start|> Crop rotation <|end|> ###Question : <|start|> Which farming practice helps prevent soil depletion and erosion? <|end|> ```@start@B <|end|> ``` farming practice helps prevent soil depletion and erosion <|end|> <|start|> crop rotation <|end|> ```@start@B <|end|> ``` <|end|>\n"
     ]
    }
   ],
   "source": [
    "# Prepare the Prompt\n",
    "question = \"What farming practice helps prevent soil erosion?\"\n",
    "prompt = chat_template_for_inference(question)  # Assuming chat_template concatenates the strings.\n",
    "\n",
    "# Encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')  # Ensure inputs are on CPU\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "# Decode the output\n",
    "text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
